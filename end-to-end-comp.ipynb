{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nimport os\n\nimport torch\nimport torchvision\nfrom torch import nn , optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T09:26:03.179052Z","iopub.execute_input":"2024-06-13T09:26:03.179420Z","iopub.status.idle":"2024-06-13T09:26:08.477590Z","shell.execute_reply.started":"2024-06-13T09:26:03.179389Z","shell.execute_reply":"2024-06-13T09:26:08.476524Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import math\nirange = range","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:26:12.791963Z","iopub.execute_input":"2024-06-13T09:26:12.794089Z","iopub.status.idle":"2024-06-13T09:26:12.800777Z","shell.execute_reply.started":"2024-06-13T09:26:12.794053Z","shell.execute_reply":"2024-06-13T09:26:12.799848Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def make_grid(tensor, nrow=8, padding=2,\n              normalize=False, range=None, scale_each=False, pad_value=0):\n    \"\"\"Make a grid of images.\n    Args:\n        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n            or a list of images all of the same size.\n        nrow (int, optional): Number of images displayed in each row of the grid.\n            The Final grid size is (B / nrow, nrow). Default is 8.\n        padding (int, optional): amount of padding. Default is 2.\n        normalize (bool, optional): If True, shift the image to the range (0, 1),\n            by subtracting the minimum and dividing by the maximum pixel value.\n        range (tuple, optional): tuple (min, max) where min and max are numbers,\n            then these numbers are used to normalize the image. By default, min and max\n            are computed from the tensor.\n        scale_each (bool, optional): If True, scale each image in the batch of\n            images separately rather than the (min, max) over all images.\n        pad_value (float, optional): Value for the padded pixels.\n    Example:\n        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n    \"\"\"\n    if not (torch.is_tensor(tensor) or\n            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n\n    # if list of tensors, convert to a 4D mini-batch Tensor\n    if isinstance(tensor, list):\n        tensor = torch.stack(tensor, dim=0)\n\n    if tensor.dim() == 2:  # single image H x W\n        tensor = tensor.view(1, tensor.size(0), tensor.size(1))\n    if tensor.dim() == 3:  # single image\n        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n            tensor = torch.cat((tensor, tensor, tensor), 0)\n        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))\n\n    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n        tensor = torch.cat((tensor, tensor, tensor), 1)\n\n    if normalize is True:\n        tensor = tensor.clone()  # avoid modifying tensor in-place\n        if range is not None:\n            assert isinstance(range, tuple), \\\n                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n\n        def norm_ip(img, min, max):\n            img.clamp_(min=min, max=max)\n            img.add_(-min).div_(max - min + 1e-5)\n\n        def norm_range(t, range):\n            if range is not None:\n                norm_ip(t, range[0], range[1])\n            else:\n                norm_ip(t, float(t.min()), float(t.max()))\n\n        if scale_each is True:\n            for t in tensor:  # loop over mini-batch dimension\n                norm_range(t, range)\n        else:\n            norm_range(tensor, range)\n\n    if tensor.size(0) == 1:\n        return tensor.squeeze()\n\n    # make the mini-batch of images into a grid\n    nmaps = tensor.size(0)\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)\n    k = 0\n    for y in irange(ymaps):\n        for x in irange(xmaps):\n            if k >= nmaps:\n                break\n            grid.narrow(1, y * height + padding, height - padding)\\\n                .narrow(2, x * width + padding, width - padding)\\\n                .copy_(tensor[k])\n            k = k + 1\n    return grid\n\n\ndef save_image(tensor, filename, nrow=8, padding=2,\n               normalize=False, range=None, scale_each=False, pad_value=0):\n    \"\"\"Save a given Tensor into an image file.\n    Args:\n        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n            saves the tensor as a grid of images by calling ``make_grid``.\n        **kwargs: Other arguments are documented in ``make_grid``.\n    \"\"\"\n    from PIL import Image\n    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n                     normalize=normalize, range=range, scale_each=scale_each)\n    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n    im = Image.fromarray(ndarr)\n    im.save(filename)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:26:13.980998Z","iopub.execute_input":"2024-06-13T09:26:13.981371Z","iopub.status.idle":"2024-06-13T09:26:14.002811Z","shell.execute_reply.started":"2024-06-13T09:26:13.981343Z","shell.execute_reply":"2024-06-13T09:26:14.001716Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"img_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.24703223,  0.24348513 , 0.26158784))\n])\n\ntrainset = datasets.CIFAR10(root='./data', train=True,download=True, transform=img_transform)\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=16,\n                                          shuffle=True, num_workers=2)\n\ntestset = datasets.CIFAR10(root='./data', train=False,download=True, transform=img_transform)\n\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=16,shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:26:16.064863Z","iopub.execute_input":"2024-06-13T09:26:16.065224Z","iopub.status.idle":"2024-06-13T09:26:21.849637Z","shell.execute_reply.started":"2024-06-13T09:26:16.065189Z","shell.execute_reply":"2024-06-13T09:26:21.848520Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 79602108.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"CHANNELS = 3\nHEIGHT = 32\nWIDTH = 32\nEPOCHS = 30\nLOG_INTERVAL = 500","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:57:56.911115Z","iopub.execute_input":"2024-06-13T09:57:56.911523Z","iopub.status.idle":"2024-06-13T09:57:56.916636Z","shell.execute_reply.started":"2024-06-13T09:57:56.911488Z","shell.execute_reply":"2024-06-13T09:57:56.915663Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class Interpolate(nn.Module):\n    def __init__(self, size, mode):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.size = size\n        self.mode = mode  \n        \n    def forward(self, x):\n        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:26:36.833827Z","iopub.execute_input":"2024-06-13T09:26:36.834172Z","iopub.status.idle":"2024-06-13T09:26:36.840112Z","shell.execute_reply.started":"2024-06-13T09:26:36.834145Z","shell.execute_reply":"2024-06-13T09:26:36.839115Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class End_to_end(nn.Module):\n    def __init__(self):\n        super(End_to_end, self).__init__()\n    \n        # Encoder\n        # TODO : try with padding = 0\n        self.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0)\n        self.bn1 = nn.BatchNorm2d(64, affine=False)\n        self.conv3 = nn.Conv2d(64, CHANNELS, kernel_size=3, stride=1, padding=1)\n        \n        # Decoder\n        #TODO : try ConvTranspose2d\n        self.interpolate = Interpolate(size=HEIGHT, mode='bilinear')\n        self.deconv1 = nn.Conv2d(CHANNELS, 64, 3, stride=1, padding=1)\n        self.deconv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64, affine=False)\n\n        self.deconv_n = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.bn_n = nn.BatchNorm2d(64, affine=False)\n\n\n        self.deconv3 = nn.ConvTranspose2d(64, CHANNELS, 3, stride=1, padding=1)\n\n        self.relu = nn.ReLU()\n        \n    def encode(self, x):\n        out = self.relu(self.conv1(x))\n        out = self.relu(self.conv2(out))\n        out = self.bn1(out)\n        return self.conv3(out)\n    def reparameterize(self, mu, logvar):\n        pass\n    \n    def decode(self, z):\n        upscaled_image = self.interpolate(z)\n        out = self.relu(self.deconv1(upscaled_image))\n        out = self.relu(self.deconv2(out))\n        out = self.bn2(out)\n        for _ in range(10):\n            out = self.relu(self.deconv_n(out))\n            out = self.bn_n(out)\n        out = self.deconv3(out)\n        final = upscaled_image + out\n        return final,out,upscaled_image\n    \n    def forward(self, x):\n        com_img = self.encode(x)\n        final,out,upscaled_image = self.decode(com_img)\n        return final, out, upscaled_image, com_img, x\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:30:50.178304Z","iopub.execute_input":"2024-06-13T09:30:50.179132Z","iopub.status.idle":"2024-06-13T09:30:50.192504Z","shell.execute_reply.started":"2024-06-13T09:30:50.179099Z","shell.execute_reply":"2024-06-13T09:30:50.191606Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"CUDA = torch.cuda.is_available()\nif CUDA:\n    model = End_to_end().cuda()\nelse :\n    model = End_to_end()\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:40:26.112168Z","iopub.execute_input":"2024-06-13T09:40:26.112856Z","iopub.status.idle":"2024-06-13T09:40:26.332170Z","shell.execute_reply.started":"2024-06-13T09:40:26.112823Z","shell.execute_reply":"2024-06-13T09:40:26.331395Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def loss_function(final_img,residual_img,upscaled_img,com_img,orig_img):\n    com_loss = nn.MSELoss(size_average=False)(orig_img, final_img)\n    rec_loss = nn.MSELoss(size_average=False)(residual_img,orig_img-upscaled_img)\n    return com_loss + rec_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:40:54.043707Z","iopub.execute_input":"2024-06-13T09:40:54.044550Z","iopub.status.idle":"2024-06-13T09:40:54.049811Z","shell.execute_reply.started":"2024-06-13T09:40:54.044515Z","shell.execute_reply":"2024-06-13T09:40:54.048611Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = Variable(data)\n        optimizer.zero_grad()\n        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n        loss = loss_function(final, residual_img, upscaled_image, com_img, orig_im)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * data.size(0)\n        if batch_idx % LOG_INTERVAL == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(data)))\n\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, train_loss / len(train_loader.dataset)))     ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:58:11.885430Z","iopub.execute_input":"2024-06-13T09:58:11.886328Z","iopub.status.idle":"2024-06-13T09:58:11.897197Z","shell.execute_reply.started":"2024-06-13T09:58:11.886284Z","shell.execute_reply":"2024-06-13T09:58:11.895874Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def test(epoch):\n  \n    model.eval()\n    test_loss = 0\n    for i, (data, _) in enumerate(test_loader):\n        data = Variable(data)\n        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).item() * data.size(0)\n        if epoch == EPOCHS and i == 0:\n#             save_image(final.data[0],'reconstruction_final',nrow=8)\n#             save_image(com_img.data[0],'com_img',nrow=8)\n            n = min(data.size(0), 6)\n            print(\"saving the image \"+str(n))\n            comparison = torch.cat([data[:n],\n              final[:n].cpu()])\n            comparison = comparison.cpu()\n#             print(comparison.data)\n            save_image(com_img[:n].data,\n                       'compressed_' + str(epoch) +'.png', nrow=n)\n            save_image(comparison.data,\n                       'reconstruction_' + str(epoch) +'.png', nrow=n)\n\n    test_loss /= len(test_loader.dataset)\n    print('====> Test set loss: {:.4f}'.format(test_loss))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:58:12.128148Z","iopub.execute_input":"2024-06-13T09:58:12.128604Z","iopub.status.idle":"2024-06-13T09:58:12.138900Z","shell.execute_reply.started":"2024-06-13T09:58:12.128573Z","shell.execute_reply":"2024-06-13T09:58:12.137540Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:58:12.479165Z","iopub.execute_input":"2024-06-13T09:58:12.479598Z","iopub.status.idle":"2024-06-13T09:58:12.490117Z","shell.execute_reply.started":"2024-06-13T09:58:12.479565Z","shell.execute_reply":"2024-06-13T09:58:12.489046Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"End_to_end(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  (conv3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (interpolate): Interpolate()\n  (deconv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (deconv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  (deconv_n): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn_n): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  (deconv3): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (relu): ReLU()\n)"},"metadata":{}}]},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    train(epoch)\n    test(epoch)\n    if epoch == EPOCHS:\n        pass\n#         sample = Variable(torch.randn(64, args.hidden_size),16)\n#         sample = model.decode(sample)\n#         print(\"saving im\")\n#         save_image(sample.data.view(64, 3, 32, 32),\n# 'sample_' + str(epoch) + '.png')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T09:58:14.635142Z","iopub.execute_input":"2024-06-13T09:58:14.635555Z","iopub.status.idle":"2024-06-13T10:21:09.436199Z","shell.execute_reply.started":"2024-06-13T09:58:14.635524Z","shell.execute_reply":"2024-06-13T10:21:09.434955Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/50000 (0%)]\tLoss: 126.118851\nTrain Epoch: 1 [8000/50000 (16%)]\tLoss: 98.546265\nTrain Epoch: 1 [16000/50000 (32%)]\tLoss: 177.142273\nTrain Epoch: 1 [24000/50000 (48%)]\tLoss: 114.054276\nTrain Epoch: 1 [32000/50000 (64%)]\tLoss: 117.828644\nTrain Epoch: 1 [40000/50000 (80%)]\tLoss: 146.150360\nTrain Epoch: 1 [48000/50000 (96%)]\tLoss: 91.173653\n====> Epoch: 1 Average loss: 2067.9632\n====> Test set loss: 6675.7213\nTrain Epoch: 2 [0/50000 (0%)]\tLoss: 124.630196\nTrain Epoch: 2 [8000/50000 (16%)]\tLoss: 109.089935\nTrain Epoch: 2 [16000/50000 (32%)]\tLoss: 115.094406\nTrain Epoch: 2 [24000/50000 (48%)]\tLoss: 104.540436\nTrain Epoch: 2 [32000/50000 (64%)]\tLoss: 142.997589\nTrain Epoch: 2 [40000/50000 (80%)]\tLoss: 123.259140\nTrain Epoch: 2 [48000/50000 (96%)]\tLoss: 134.290833\n====> Epoch: 2 Average loss: 2026.5839\n====> Test set loss: 7732.0445\nTrain Epoch: 3 [0/50000 (0%)]\tLoss: 158.209778\nTrain Epoch: 3 [8000/50000 (16%)]\tLoss: 139.417999\nTrain Epoch: 3 [16000/50000 (32%)]\tLoss: 83.738007\nTrain Epoch: 3 [24000/50000 (48%)]\tLoss: 137.175079\nTrain Epoch: 3 [32000/50000 (64%)]\tLoss: 119.951187\nTrain Epoch: 3 [40000/50000 (80%)]\tLoss: 131.956360\nTrain Epoch: 3 [48000/50000 (96%)]\tLoss: 154.883331\n====> Epoch: 3 Average loss: 1963.8566\n====> Test set loss: 6893.1789\nTrain Epoch: 4 [0/50000 (0%)]\tLoss: 146.899780\nTrain Epoch: 4 [8000/50000 (16%)]\tLoss: 116.790344\nTrain Epoch: 4 [16000/50000 (32%)]\tLoss: 139.573334\nTrain Epoch: 4 [24000/50000 (48%)]\tLoss: 190.223358\nTrain Epoch: 4 [32000/50000 (64%)]\tLoss: 92.916176\nTrain Epoch: 4 [40000/50000 (80%)]\tLoss: 116.240509\nTrain Epoch: 4 [48000/50000 (96%)]\tLoss: 114.411980\n====> Epoch: 4 Average loss: 1952.0838\n====> Test set loss: 7311.2625\nTrain Epoch: 5 [0/50000 (0%)]\tLoss: 88.749359\nTrain Epoch: 5 [8000/50000 (16%)]\tLoss: 111.880722\nTrain Epoch: 5 [16000/50000 (32%)]\tLoss: 111.158897\nTrain Epoch: 5 [24000/50000 (48%)]\tLoss: 100.011963\nTrain Epoch: 5 [32000/50000 (64%)]\tLoss: 96.657166\nTrain Epoch: 5 [40000/50000 (80%)]\tLoss: 104.897797\nTrain Epoch: 5 [48000/50000 (96%)]\tLoss: 94.667770\n====> Epoch: 5 Average loss: 1899.3520\n====> Test set loss: 7785.0389\nTrain Epoch: 6 [0/50000 (0%)]\tLoss: 130.067383\nTrain Epoch: 6 [8000/50000 (16%)]\tLoss: 91.415062\nTrain Epoch: 6 [16000/50000 (32%)]\tLoss: 113.468056\nTrain Epoch: 6 [24000/50000 (48%)]\tLoss: 125.599663\nTrain Epoch: 6 [32000/50000 (64%)]\tLoss: 159.655624\nTrain Epoch: 6 [40000/50000 (80%)]\tLoss: 127.168259\nTrain Epoch: 6 [48000/50000 (96%)]\tLoss: 101.015648\n====> Epoch: 6 Average loss: 1856.0083\n====> Test set loss: 5791.9632\nTrain Epoch: 7 [0/50000 (0%)]\tLoss: 104.933777\nTrain Epoch: 7 [8000/50000 (16%)]\tLoss: 126.137886\nTrain Epoch: 7 [16000/50000 (32%)]\tLoss: 127.531715\nTrain Epoch: 7 [24000/50000 (48%)]\tLoss: 108.471741\nTrain Epoch: 7 [32000/50000 (64%)]\tLoss: 125.810730\nTrain Epoch: 7 [40000/50000 (80%)]\tLoss: 125.202591\nTrain Epoch: 7 [48000/50000 (96%)]\tLoss: 105.081169\n====> Epoch: 7 Average loss: 1821.5770\n====> Test set loss: 8700.4403\nTrain Epoch: 8 [0/50000 (0%)]\tLoss: 105.125748\nTrain Epoch: 8 [8000/50000 (16%)]\tLoss: 87.380493\nTrain Epoch: 8 [16000/50000 (32%)]\tLoss: 105.501663\nTrain Epoch: 8 [24000/50000 (48%)]\tLoss: 130.842346\nTrain Epoch: 8 [32000/50000 (64%)]\tLoss: 99.547638\nTrain Epoch: 8 [40000/50000 (80%)]\tLoss: 130.341370\nTrain Epoch: 8 [48000/50000 (96%)]\tLoss: 125.664948\n====> Epoch: 8 Average loss: 1802.4760\n====> Test set loss: 9551.8172\nTrain Epoch: 9 [0/50000 (0%)]\tLoss: 144.564575\nTrain Epoch: 9 [8000/50000 (16%)]\tLoss: 84.706757\nTrain Epoch: 9 [16000/50000 (32%)]\tLoss: 108.038589\nTrain Epoch: 9 [24000/50000 (48%)]\tLoss: 107.666336\nTrain Epoch: 9 [32000/50000 (64%)]\tLoss: 92.149109\nTrain Epoch: 9 [40000/50000 (80%)]\tLoss: 115.939621\nTrain Epoch: 9 [48000/50000 (96%)]\tLoss: 97.123863\n====> Epoch: 9 Average loss: 1788.3408\n====> Test set loss: 8710.8126\nTrain Epoch: 10 [0/50000 (0%)]\tLoss: 94.094704\nTrain Epoch: 10 [8000/50000 (16%)]\tLoss: 117.631111\nTrain Epoch: 10 [16000/50000 (32%)]\tLoss: 92.497330\nTrain Epoch: 10 [24000/50000 (48%)]\tLoss: 146.412842\nTrain Epoch: 10 [32000/50000 (64%)]\tLoss: 136.761658\nTrain Epoch: 10 [40000/50000 (80%)]\tLoss: 124.792526\nTrain Epoch: 10 [48000/50000 (96%)]\tLoss: 150.916534\n====> Epoch: 10 Average loss: 1794.0060\n====> Test set loss: 12941.5256\nTrain Epoch: 11 [0/50000 (0%)]\tLoss: 153.447449\nTrain Epoch: 11 [8000/50000 (16%)]\tLoss: 109.444611\nTrain Epoch: 11 [16000/50000 (32%)]\tLoss: 106.833267\nTrain Epoch: 11 [24000/50000 (48%)]\tLoss: 1022.784302\nTrain Epoch: 11 [32000/50000 (64%)]\tLoss: 244.669144\nTrain Epoch: 11 [40000/50000 (80%)]\tLoss: 246.220459\nTrain Epoch: 11 [48000/50000 (96%)]\tLoss: 113.710907\n====> Epoch: 11 Average loss: 2950.5442\n====> Test set loss: 5950.2243\nTrain Epoch: 12 [0/50000 (0%)]\tLoss: 169.874557\nTrain Epoch: 12 [8000/50000 (16%)]\tLoss: 164.373474\nTrain Epoch: 12 [16000/50000 (32%)]\tLoss: 133.170105\nTrain Epoch: 12 [24000/50000 (48%)]\tLoss: 123.040657\nTrain Epoch: 12 [32000/50000 (64%)]\tLoss: 92.362549\nTrain Epoch: 12 [40000/50000 (80%)]\tLoss: 134.891663\nTrain Epoch: 12 [48000/50000 (96%)]\tLoss: 121.162720\n====> Epoch: 12 Average loss: 2074.2871\n====> Test set loss: 5538.7112\nTrain Epoch: 13 [0/50000 (0%)]\tLoss: 91.069817\nTrain Epoch: 13 [8000/50000 (16%)]\tLoss: 92.918427\nTrain Epoch: 13 [16000/50000 (32%)]\tLoss: 113.988098\nTrain Epoch: 13 [24000/50000 (48%)]\tLoss: 118.888069\nTrain Epoch: 13 [32000/50000 (64%)]\tLoss: 85.495415\nTrain Epoch: 13 [40000/50000 (80%)]\tLoss: 105.095673\nTrain Epoch: 13 [48000/50000 (96%)]\tLoss: 110.412140\n====> Epoch: 13 Average loss: 1905.4110\n====> Test set loss: 6834.9435\nTrain Epoch: 14 [0/50000 (0%)]\tLoss: 92.826881\nTrain Epoch: 14 [8000/50000 (16%)]\tLoss: 125.975525\nTrain Epoch: 14 [16000/50000 (32%)]\tLoss: 86.445618\nTrain Epoch: 14 [24000/50000 (48%)]\tLoss: 78.919174\nTrain Epoch: 14 [32000/50000 (64%)]\tLoss: 76.099289\nTrain Epoch: 14 [40000/50000 (80%)]\tLoss: 94.905029\nTrain Epoch: 14 [48000/50000 (96%)]\tLoss: 116.348557\n====> Epoch: 14 Average loss: 1835.6880\n====> Test set loss: 5912.6662\nTrain Epoch: 15 [0/50000 (0%)]\tLoss: 86.975891\nTrain Epoch: 15 [8000/50000 (16%)]\tLoss: 112.447662\nTrain Epoch: 15 [16000/50000 (32%)]\tLoss: 117.711922\nTrain Epoch: 15 [24000/50000 (48%)]\tLoss: 119.243256\nTrain Epoch: 15 [32000/50000 (64%)]\tLoss: 117.470932\nTrain Epoch: 15 [40000/50000 (80%)]\tLoss: 96.317078\nTrain Epoch: 15 [48000/50000 (96%)]\tLoss: 140.711975\n====> Epoch: 15 Average loss: 1781.8793\n====> Test set loss: 9099.9306\nTrain Epoch: 16 [0/50000 (0%)]\tLoss: 111.366592\nTrain Epoch: 16 [8000/50000 (16%)]\tLoss: 158.818756\nTrain Epoch: 16 [16000/50000 (32%)]\tLoss: 129.404709\nTrain Epoch: 16 [24000/50000 (48%)]\tLoss: 95.625641\nTrain Epoch: 16 [32000/50000 (64%)]\tLoss: 112.650528\nTrain Epoch: 16 [40000/50000 (80%)]\tLoss: 107.617821\nTrain Epoch: 16 [48000/50000 (96%)]\tLoss: 115.415718\n====> Epoch: 16 Average loss: 1757.6827\n====> Test set loss: 8138.3606\nTrain Epoch: 17 [0/50000 (0%)]\tLoss: 94.941422\nTrain Epoch: 17 [8000/50000 (16%)]\tLoss: 121.866928\nTrain Epoch: 17 [16000/50000 (32%)]\tLoss: 109.809662\nTrain Epoch: 17 [24000/50000 (48%)]\tLoss: 93.558762\nTrain Epoch: 17 [32000/50000 (64%)]\tLoss: 110.245621\nTrain Epoch: 17 [40000/50000 (80%)]\tLoss: 133.134796\nTrain Epoch: 17 [48000/50000 (96%)]\tLoss: 88.524521\n====> Epoch: 17 Average loss: 1728.1577\n====> Test set loss: 7347.8841\nTrain Epoch: 18 [0/50000 (0%)]\tLoss: 103.457291\nTrain Epoch: 18 [8000/50000 (16%)]\tLoss: 132.456100\nTrain Epoch: 18 [16000/50000 (32%)]\tLoss: 118.598068\nTrain Epoch: 18 [24000/50000 (48%)]\tLoss: 100.435898\nTrain Epoch: 18 [32000/50000 (64%)]\tLoss: 98.044693\nTrain Epoch: 18 [40000/50000 (80%)]\tLoss: 106.680420\nTrain Epoch: 18 [48000/50000 (96%)]\tLoss: 114.570923\n====> Epoch: 18 Average loss: 1712.3865\n====> Test set loss: 11835.7467\nTrain Epoch: 19 [0/50000 (0%)]\tLoss: 105.175499\nTrain Epoch: 19 [8000/50000 (16%)]\tLoss: 99.614441\nTrain Epoch: 19 [16000/50000 (32%)]\tLoss: 96.661270\nTrain Epoch: 19 [24000/50000 (48%)]\tLoss: 107.003677\nTrain Epoch: 19 [32000/50000 (64%)]\tLoss: 114.081467\nTrain Epoch: 19 [40000/50000 (80%)]\tLoss: 99.112061\nTrain Epoch: 19 [48000/50000 (96%)]\tLoss: 108.757980\n====> Epoch: 19 Average loss: 1684.2322\n====> Test set loss: 8681.2399\nTrain Epoch: 20 [0/50000 (0%)]\tLoss: 105.606186\nTrain Epoch: 20 [8000/50000 (16%)]\tLoss: 85.810631\nTrain Epoch: 20 [16000/50000 (32%)]\tLoss: 97.437714\nTrain Epoch: 20 [24000/50000 (48%)]\tLoss: 107.963333\nTrain Epoch: 20 [32000/50000 (64%)]\tLoss: 114.388458\nTrain Epoch: 20 [40000/50000 (80%)]\tLoss: 98.304184\nTrain Epoch: 20 [48000/50000 (96%)]\tLoss: 74.042946\n====> Epoch: 20 Average loss: 1669.2903\n====> Test set loss: 8993.3458\nTrain Epoch: 21 [0/50000 (0%)]\tLoss: 92.092674\nTrain Epoch: 21 [8000/50000 (16%)]\tLoss: 110.748405\nTrain Epoch: 21 [16000/50000 (32%)]\tLoss: 86.558678\nTrain Epoch: 21 [24000/50000 (48%)]\tLoss: 110.623489\nTrain Epoch: 21 [32000/50000 (64%)]\tLoss: 127.657135\nTrain Epoch: 21 [40000/50000 (80%)]\tLoss: 101.476547\nTrain Epoch: 21 [48000/50000 (96%)]\tLoss: 89.440331\n====> Epoch: 21 Average loss: 1655.2721\n====> Test set loss: 9304.2827\nTrain Epoch: 22 [0/50000 (0%)]\tLoss: 75.029877\nTrain Epoch: 22 [8000/50000 (16%)]\tLoss: 135.499756\nTrain Epoch: 22 [16000/50000 (32%)]\tLoss: 75.847450\nTrain Epoch: 22 [24000/50000 (48%)]\tLoss: 89.112671\nTrain Epoch: 22 [32000/50000 (64%)]\tLoss: 94.072891\nTrain Epoch: 22 [40000/50000 (80%)]\tLoss: 103.500519\nTrain Epoch: 22 [48000/50000 (96%)]\tLoss: 78.769165\n====> Epoch: 22 Average loss: 1640.8445\n====> Test set loss: 6859.4493\nTrain Epoch: 23 [0/50000 (0%)]\tLoss: 89.804901\nTrain Epoch: 23 [8000/50000 (16%)]\tLoss: 91.560158\nTrain Epoch: 23 [16000/50000 (32%)]\tLoss: 105.329483\nTrain Epoch: 23 [24000/50000 (48%)]\tLoss: 99.225983\nTrain Epoch: 23 [32000/50000 (64%)]\tLoss: 131.782501\nTrain Epoch: 23 [40000/50000 (80%)]\tLoss: 83.037865\nTrain Epoch: 23 [48000/50000 (96%)]\tLoss: 105.024857\n====> Epoch: 23 Average loss: 1633.0330\n====> Test set loss: 11477.4767\nTrain Epoch: 24 [0/50000 (0%)]\tLoss: 92.884567\nTrain Epoch: 24 [8000/50000 (16%)]\tLoss: 93.681534\nTrain Epoch: 24 [16000/50000 (32%)]\tLoss: 106.145302\nTrain Epoch: 24 [24000/50000 (48%)]\tLoss: 109.361603\nTrain Epoch: 24 [32000/50000 (64%)]\tLoss: 88.283356\nTrain Epoch: 24 [40000/50000 (80%)]\tLoss: 79.187012\nTrain Epoch: 24 [48000/50000 (96%)]\tLoss: 111.771072\n====> Epoch: 24 Average loss: 1600.1449\n====> Test set loss: 9001.5117\nTrain Epoch: 25 [0/50000 (0%)]\tLoss: 85.667015\nTrain Epoch: 25 [8000/50000 (16%)]\tLoss: 113.822319\nTrain Epoch: 25 [16000/50000 (32%)]\tLoss: 93.318039\nTrain Epoch: 25 [24000/50000 (48%)]\tLoss: 74.674210\nTrain Epoch: 25 [32000/50000 (64%)]\tLoss: 103.572083\nTrain Epoch: 25 [40000/50000 (80%)]\tLoss: 90.383896\nTrain Epoch: 25 [48000/50000 (96%)]\tLoss: 83.298004\n====> Epoch: 25 Average loss: 1607.1421\n====> Test set loss: 11802.6443\nTrain Epoch: 26 [0/50000 (0%)]\tLoss: 108.297073\nTrain Epoch: 26 [8000/50000 (16%)]\tLoss: 68.963287\nTrain Epoch: 26 [16000/50000 (32%)]\tLoss: 105.232162\nTrain Epoch: 26 [24000/50000 (48%)]\tLoss: 86.836166\nTrain Epoch: 26 [32000/50000 (64%)]\tLoss: 116.288895\nTrain Epoch: 26 [40000/50000 (80%)]\tLoss: 104.826973\nTrain Epoch: 26 [48000/50000 (96%)]\tLoss: 79.794350\n====> Epoch: 26 Average loss: 1585.6778\n====> Test set loss: 9198.9259\nTrain Epoch: 27 [0/50000 (0%)]\tLoss: 89.507080\nTrain Epoch: 27 [8000/50000 (16%)]\tLoss: 103.786484\nTrain Epoch: 27 [16000/50000 (32%)]\tLoss: 94.386856\nTrain Epoch: 27 [24000/50000 (48%)]\tLoss: 79.997314\nTrain Epoch: 27 [32000/50000 (64%)]\tLoss: 72.732452\nTrain Epoch: 27 [40000/50000 (80%)]\tLoss: 116.537964\nTrain Epoch: 27 [48000/50000 (96%)]\tLoss: 112.910019\n====> Epoch: 27 Average loss: 1571.8007\n====> Test set loss: 8390.9201\nTrain Epoch: 28 [0/50000 (0%)]\tLoss: 84.658752\nTrain Epoch: 28 [8000/50000 (16%)]\tLoss: 91.759354\nTrain Epoch: 28 [16000/50000 (32%)]\tLoss: 86.197754\nTrain Epoch: 28 [24000/50000 (48%)]\tLoss: 109.663101\nTrain Epoch: 28 [32000/50000 (64%)]\tLoss: 75.233536\nTrain Epoch: 28 [40000/50000 (80%)]\tLoss: 86.682487\nTrain Epoch: 28 [48000/50000 (96%)]\tLoss: 175.826767\n====> Epoch: 28 Average loss: 1563.6426\n====> Test set loss: 5839.9264\nTrain Epoch: 29 [0/50000 (0%)]\tLoss: 82.590508\nTrain Epoch: 29 [8000/50000 (16%)]\tLoss: 117.050316\nTrain Epoch: 29 [16000/50000 (32%)]\tLoss: 127.020691\nTrain Epoch: 29 [24000/50000 (48%)]\tLoss: 101.980545\nTrain Epoch: 29 [32000/50000 (64%)]\tLoss: 106.958755\nTrain Epoch: 29 [40000/50000 (80%)]\tLoss: 103.964600\nTrain Epoch: 29 [48000/50000 (96%)]\tLoss: 90.140930\n====> Epoch: 29 Average loss: 1565.3671\n====> Test set loss: 6988.7176\nTrain Epoch: 30 [0/50000 (0%)]\tLoss: 92.514305\nTrain Epoch: 30 [8000/50000 (16%)]\tLoss: 93.018188\nTrain Epoch: 30 [16000/50000 (32%)]\tLoss: 113.649963\nTrain Epoch: 30 [24000/50000 (48%)]\tLoss: 79.395203\nTrain Epoch: 30 [32000/50000 (64%)]\tLoss: 127.441643\nTrain Epoch: 30 [40000/50000 (80%)]\tLoss: 88.605957\nTrain Epoch: 30 [48000/50000 (96%)]\tLoss: 109.824005\n====> Epoch: 30 Average loss: 1550.7958\nsaving the image 6\n====> Test set loss: 5765.6688\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), './net.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:22:17.691911Z","iopub.execute_input":"2024-06-13T10:22:17.692891Z","iopub.status.idle":"2024-06-13T10:22:17.703997Z","shell.execute_reply.started":"2024-06-13T10:22:17.692850Z","shell.execute_reply":"2024-06-13T10:22:17.703141Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('net.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:22:46.559861Z","iopub.execute_input":"2024-06-13T10:22:46.560327Z","iopub.status.idle":"2024-06-13T10:22:46.573508Z","shell.execute_reply.started":"2024-06-13T10:22:46.560288Z","shell.execute_reply":"2024-06-13T10:22:46.572574Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def save_images():\n    epoch = EPOCHS\n    model.eval()\n    test_loss = 0\n    for i, (data, _) in enumerate(test_loader):\n        data = Variable(data)\n        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).item() * data.size(0)\n        if i == 3:\n#             save_image(final.data[0],'reconstruction_final',nrow=8)\n#             save_image(com_img.data[0],'com_img',nrow=8)\n            n = min(data.size(0), 6)\n            print(\"saving the image \"+str(n))\n            comparison = torch.cat([data[:n],\n              final[:n].cpu()])\n            comparison = comparison.cpu()\n#             print(comparison.data)\n            save_image(com_img[:1].data,\n                         'compressed_' + str(i) +'.png', nrow=n)\n            save_image(final[:1].data,\n                        'final_' + str(epoch) +'.png', nrow=n)\n            save_image(orig_im[:1].data,\n                        'original_' + str(epoch) +'.png', nrow=n)\n    test_loss /= len(test_loader.dataset)\n    print('====> Test set loss: {:.4f}'.format(test_loss))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:44.448581Z","iopub.execute_input":"2024-06-13T10:24:44.449234Z","iopub.status.idle":"2024-06-13T10:24:44.459596Z","shell.execute_reply.started":"2024-06-13T10:24:44.449199Z","shell.execute_reply":"2024-06-13T10:24:44.458645Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"save_images()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:45.525419Z","iopub.execute_input":"2024-06-13T10:24:45.525820Z","iopub.status.idle":"2024-06-13T10:24:48.648536Z","shell.execute_reply.started":"2024-06-13T10:24:45.525788Z","shell.execute_reply":"2024-06-13T10:24:48.647387Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"saving the image 6\n====> Test set loss: 5765.6688\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}